{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #### REMOVE THIS LINE WHEN CUDA CONFIG IS FIXED\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import gym_snake\n",
    "import json\n",
    "import importlib\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from utils.Buffer import ReplayBuffer\n",
    "from rl.models import get_policy_architecture, get_value_architecture\n",
    "from algos.PPO import PPO_agent\n",
    "from algos.DQN import DQN_agent\n",
    "\n",
    "# %load_ext line_profiler\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tetris = importlib.import_module('pytris-effect.src.gameui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'lunarlander'\n",
    "action = 'train'\n",
    "algo = ('DQN', 'Dueling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_fp = os.path.join('..', 'configs', run_name + '.json')\n",
    "with open(cfg_fp, 'r') as f:\n",
    "    config = json.load(f)\n",
    "ckpt_folder = os.path.join('..', 'checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = config['env']\n",
    "if run_name == 'tetris':\n",
    "    env = tetris.GameUI(graphic_mode=False, its_per_sec=2, sec_per_tick=0.5)\n",
    "else:\n",
    "    env = gym.make(env_name).env if 'use_raw_env' in config else gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(arr):\n",
    "    scaling = 30\n",
    "    data = np.zeros((scaling*arr.shape[0], scaling*arr.shape[1], 3), dtype=np.uint8)\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            for k in range(data.shape[2]):\n",
    "                data[i,j,k] = arr[i//scaling,j//scaling,k]\n",
    "    img = Image.fromarray(data, 'RGB')\n",
    "    # img.save('my.png')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'evaluate':\n",
    "    %lprun -f env.drawMatrix env.drawMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_img(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    action = 1\n",
    "    obs, reward, dn, info = env.step(action)\n",
    "    show_img(obs)\n",
    "    print(reward, dn, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_step():\n",
    "    _, _, dn, _ = env.step(random.choice(range(7)))\n",
    "    if dn:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit do_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -f env.get_obs do_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_policy_architecture(env_name, algo=algo)\n",
    "if 'DQN' in algo:\n",
    "    target = tf.keras.models.clone_model(model)\n",
    "else:\n",
    "    value = get_value_architecture(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'DQN' in algo:\n",
    "    agent = DQN_agent(\n",
    "        model,\n",
    "        # (TODO): Move args for ReplayBuffer into DQN\n",
    "        ReplayBuffer(config.get(\"max_buf_size\", 20000), mode='proportional'),\n",
    "        target=target,\n",
    "        env=env,\n",
    "        mode=('DDQN', 'PER'), # 'PER'\n",
    "        learning_rate=config['learning_rate'],\n",
    "        batch_size=config['batch_size'],\n",
    "        update_steps=1,\n",
    "        update_freq=1,\n",
    "        multistep=10,\n",
    "        alpha=2.0,\n",
    "        beta=1.0,\n",
    "        gamma=0.99,\n",
    "        delta=0.0001,\n",
    "        env_name=config['env_name'],\n",
    "        algo_name='DQN',\n",
    "        ckpt_folder=ckpt_folder,\n",
    "        # run_name='snake-DQN-23-15-44'\n",
    "    )\n",
    "elif 'PPO' in algo:\n",
    "    agent = PPO_agent(\n",
    "        model,\n",
    "        value,\n",
    "        env=env,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        minibatch_size=config['minibatch_size'],\n",
    "        epsilon=0.1,\n",
    "        gamma=1.0,\n",
    "        env_name=config['env_name'],\n",
    "        #run_name='snake-PPO-mpi8-09-01-21-run2',\n",
    "        ckpt_folder=ckpt_folder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = config['t_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_from_checkpoint()\n",
    "hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:01<00:00, 13.09it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f8a223296744c05bcb2487295161bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] Average reward: -465.3809736294188\n",
      "Predicted reward: [[-12.680162 -15.626064 -18.826466  -7.879521]]\n",
      "Buffer size: 2151\n",
      "Saving to checkpoint...\n",
      "[10] Average reward: -227.5596353392138\n",
      "Predicted reward: [[-14.222624  -15.390877  -18.503788   -2.0153906]]\n",
      "Buffer size: 2520\n",
      "Saving to checkpoint...\n",
      "[15] Average reward: -78.67672498286005\n",
      "Predicted reward: [[-18.793747 -20.307802 -20.409088  -5.594268]]\n",
      "Buffer size: 2941\n",
      "Saving to checkpoint...\n",
      "[20] Average reward: -281.8447637611816\n",
      "Predicted reward: [[-19.25713  -19.912949 -13.473327 -15.842008]]\n",
      "Buffer size: 3397\n",
      "Saving to checkpoint...\n",
      "[25] Average reward: -380.6413240769174\n",
      "Predicted reward: [[-13.954082 -11.887753  -4.749765  -7.650996]]\n",
      "Buffer size: 4075\n",
      "Saving to checkpoint...\n",
      "[30] Average reward: -264.05091408721404\n",
      "Predicted reward: [[-15.915034 -10.615066 -22.963634 -16.18062 ]]\n",
      "Buffer size: 5130\n",
      "Saving to checkpoint...\n",
      "[35] Average reward: -296.3561078428022\n",
      "Predicted reward: [[-3.3870919 -0.5670075  2.2330077 -1.8784828]]\n",
      "Buffer size: 6089\n",
      "Saving to checkpoint...\n",
      "[40] Average reward: -21.102726305099317\n",
      "Predicted reward: [[ -4.2591085  -2.4740915 -11.10922   -10.444446 ]]\n",
      "Buffer size: 8072\n",
      "Saving to checkpoint...\n",
      "[45] Average reward: -56.594295149413554\n",
      "Predicted reward: [[-1.7346591 -2.1944194 -4.079475  -2.644568 ]]\n",
      "Buffer size: 10208\n",
      "Saving to checkpoint...\n",
      "[50] Average reward: -49.17677428801062\n",
      "Predicted reward: [[ -8.056837    -6.417172     0.72586524 -14.486352  ]]\n",
      "Buffer size: 12273\n",
      "Saving to checkpoint...\n",
      "[55] Average reward: -58.60574984467818\n",
      "Predicted reward: [[-2.3734913 -4.6700726 10.232132  -3.542679 ]]\n",
      "Buffer size: 14384\n",
      "Saving to checkpoint...\n",
      "[60] Average reward: -34.84366872864454\n",
      "Predicted reward: [[-10.615215   -7.9656687   1.7661961 -14.762833 ]]\n",
      "Buffer size: 16525\n",
      "Saving to checkpoint...\n",
      "[65] Average reward: -20.99846548147783\n",
      "Predicted reward: [[-18.114384  -22.315403  -23.915888   -7.4071727]]\n",
      "Buffer size: 18980\n",
      "Saving to checkpoint...\n",
      "[70] Average reward: -28.567017581510402\n",
      "Predicted reward: [[ -9.036018  -15.645813   -3.5683742  -5.351003 ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[75] Average reward: -2.706081993675826\n",
      "Predicted reward: [[-2.0246549   0.48922336  8.456826   -5.852437  ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[80] Average reward: -45.90412891368844\n",
      "Predicted reward: [[ 9.544571  9.551542 18.302088  6.178929]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[85] Average reward: -47.459000939567616\n",
      "Predicted reward: [[ -7.94627   -6.242331  -9.186504 -10.924601]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[90] Average reward: 26.35056689132702\n",
      "Predicted reward: [[-1.7479292 -4.770873  -2.9182224  2.1596808]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[95] Average reward: -7.3679807079157085\n",
      "Predicted reward: [[ -4.828352    -9.755894   -10.413769     0.15656996]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[100] Average reward: 29.686500156931572\n",
      "Predicted reward: [[-5.8240733 -6.839424  -5.7793193 -3.4049006]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[105] Average reward: -6.413850267738636\n",
      "Predicted reward: [[11.749463 10.705238 13.63752   9.508231]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[110] Average reward: 54.323044121941486\n",
      "Predicted reward: [[12.955559  15.0804405 14.687716   8.839352 ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[115] Average reward: 46.75244934579534\n",
      "Predicted reward: [[ 1.6461886  -0.5733582   0.40502703  6.454125  ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[120] Average reward: 5.342870487755473\n",
      "Predicted reward: [[-1.0494589 -3.145836  -6.37442    4.772003 ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[125] Average reward: 51.873901330814896\n",
      "Predicted reward: [[-5.6056695 -4.4078803 -6.701626  -8.129763 ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[130] Average reward: 47.01653770742432\n",
      "Predicted reward: [[21.189314 22.017874 22.530077 18.413227]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[135] Average reward: 61.99186868389994\n",
      "Predicted reward: [[9.718365  7.7147894 9.100863  9.787597 ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[140] Average reward: 69.5393084752864\n",
      "Predicted reward: [[ 0.7678344 -1.3909549 -3.8303514  3.7814198]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[145] Average reward: 25.79187379359912\n",
      "Predicted reward: [[ 1.6958559 -1.624557  -6.0329814  3.3866131]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[150] Average reward: 39.87637664085388\n",
      "Predicted reward: [[-7.968536  -8.063626  -8.812352  -5.7625732]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[155] Average reward: 151.31610928785966\n",
      "Predicted reward: [[15.391982 17.944027 15.549508 13.955585]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[160] Average reward: 57.08028125046518\n",
      "Predicted reward: [[27.890491 28.26428  28.832127 27.067661]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[165] Average reward: 74.28848860601441\n",
      "Predicted reward: [[-0.3650298  0.9743037 -2.6736994 -2.7679472]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[170] Average reward: 24.614449690642257\n",
      "Predicted reward: [[22.660473 24.532257 21.781687 19.453114]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[175] Average reward: 54.8001721457091\n",
      "Predicted reward: [[-3.8639212 -4.87406   -7.5658803 -3.6434746]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[180] Average reward: 71.00449107300972\n",
      "Predicted reward: [[28.762627 31.796694 29.239832 26.754932]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[185] Average reward: 110.6692735326287\n",
      "Predicted reward: [[15.374649 12.72994  12.802542 16.671806]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[190] Average reward: 112.08940108019581\n",
      "Predicted reward: [[29.43897  31.846886 27.799883 26.494278]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[195] Average reward: -31.168701303452643\n",
      "Predicted reward: [[39.29991  43.671795 43.146473 39.53536 ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[200] Average reward: 34.40134376414707\n",
      "Predicted reward: [[34.75944  40.062435 35.862152 34.094982]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[205] Average reward: 23.84334758940225\n",
      "Predicted reward: [[14.529917 11.934763  9.312298 15.825253]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[210] Average reward: 104.88087453721104\n",
      "Predicted reward: [[26.318773 20.863031 20.879665 26.95957 ]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[215] Average reward: 92.70993302504891\n",
      "Predicted reward: [[18.163134 20.372108 12.882135 14.103938]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n",
      "[220] Average reward: 85.6269814456734\n",
      "Predicted reward: [[42.625942 35.9643   41.284466 42.359344]]\n",
      "Buffer size: 20000\n",
      "Saving to checkpoint...\n"
     ]
    }
   ],
   "source": [
    "if action == 'train':\n",
    "    if 'DQN' in algo:\n",
    "        # fill buffer with some random samples\n",
    "        for i in tqdm(range(25)):\n",
    "            agent.collect_rollout(t_max=t_max, policy=lambda x: np.random.choice(2), train=False, display=True)\n",
    "        hist += agent.train(epochs=config['train_epochs'], t_max=t_max, display=True)\n",
    "    elif 'PPO' in algo:\n",
    "        agent.train(epochs=config['train_epochs'], t_max=t_max, buf_size=3000, min_buf_size=600, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.get_model(agent.preprocess(env.reset())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rollout(t_max, env, close=True):\n",
    "    import sys\n",
    "    obs = agent.preprocess(env.reset())\n",
    "    reward = 0\n",
    "    for i in range(t_max):\n",
    "        # print(agent.get_policy(obs))\n",
    "        # act = agent.get_action(obs, greedy=True)[0]\n",
    "        act = agent.get_action(obs, mode='greedy')[0][0]\n",
    "        obs, r, dn, info = env.step(agent.action_wrapper(act))\n",
    "        env.render()\n",
    "        print(act, file=sys.stderr)\n",
    "        time.sleep(0.05)\n",
    "        obs = agent.preprocess(obs)\n",
    "        reward += r\n",
    "        if dn:\n",
    "            break\n",
    "\n",
    "    print(\"Total reward: {}\".format(reward), file=sys.stderr)\n",
    "    if close: env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'test':\n",
    "    test_rollout(10000, env, close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.train(4, t_max=500, min_buf_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f agent.train agent.train(1, t_max=500, buf_size=2000, min_buf_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
