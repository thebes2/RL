{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #### REMOVE THIS LINE WHEN CUDA CONFIG IS FIXED\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import gym_snake\n",
    "import json\n",
    "import importlib\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from utils.Buffer import ReplayBuffer\n",
    "from rl.models import get_policy_architecture, get_value_architecture\n",
    "from algos.PPO import PPO_agent\n",
    "from algos.DQN import DQN_agent\n",
    "\n",
    "# %load_ext line_profiler\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tetris = importlib.import_module('pytris-effect.src.gameui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'cartpole'\n",
    "action = 'train'\n",
    "algo = 'DQN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_fp = os.path.join('..', 'configs', run_name + '.json')\n",
    "with open(cfg_fp, 'r') as f:\n",
    "    config = json.load(f)\n",
    "ckpt_folder = os.path.join('..', 'checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = config['env']\n",
    "if run_name == 'tetris':\n",
    "    env = tetris.GameUI(graphic_mode=False, its_per_sec=2, sec_per_tick=0.5)\n",
    "else:\n",
    "    env = gym.make(env_name).env if 'use_raw_env' in config else gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(arr):\n",
    "    scaling = 30\n",
    "    data = np.zeros((scaling*arr.shape[0], scaling*arr.shape[1], 3), dtype=np.uint8)\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            for k in range(data.shape[2]):\n",
    "                data[i,j,k] = arr[i//scaling,j//scaling,k]\n",
    "    img = Image.fromarray(data, 'RGB')\n",
    "    # img.save('my.png')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'evaluate':\n",
    "    %lprun -f env.drawMatrix env.drawMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_img(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    action = 1\n",
    "    obs, reward, dn, info = env.step(action)\n",
    "    show_img(obs)\n",
    "    print(reward, dn, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_step():\n",
    "    _, _, dn, _ = env.step(random.choice(range(7)))\n",
    "    if dn:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit do_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -f env.get_obs do_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "model = get_policy_architecture(env_name, algo=algo)\n",
    "target = tf.keras.models.clone_model(model)\n",
    "# value = get_value_architecture(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if algo == 'DQN':\n",
    "    agent = DQN_agent(\n",
    "        model,\n",
    "        ReplayBuffer(),\n",
    "        target=target,\n",
    "        env=env,\n",
    "        beta=0.1,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        batch_size=config['batch_size'],\n",
    "        update_steps=10,\n",
    "        env_name=config['env_name'],\n",
    "        algo_name='DQN',\n",
    "        ckpt_folder=ckpt_folder\n",
    "    )\n",
    "elif algo == 'PPO':\n",
    "    agent = PPO_agent(\n",
    "        model,\n",
    "        value,\n",
    "        env=env,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        minibatch_size=config['minibatch_size'],\n",
    "        epsilon=0.1,\n",
    "        gamma=0.95,\n",
    "        env_name=config['env_name'],\n",
    "        #run_name='snake-PPO-mpi8-09-01-21-run2',\n",
    "        ckpt_folder=ckpt_folder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = config['t_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_from_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea586420f2a4516be80bdf371bb272c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] Average reward: 25.6\n",
      "Predicted reward: [[1.755229 1.726048]]\n",
      "Saving to checkpoint...\n",
      "[10] Average reward: 24.0\n",
      "Predicted reward: [[2.6093917 2.6137452]]\n",
      "Saving to checkpoint...\n",
      "[15] Average reward: 17.4\n",
      "Predicted reward: [[3.3616552 3.409727 ]]\n",
      "Saving to checkpoint...\n",
      "[20] Average reward: 38.2\n",
      "Predicted reward: [[4.833555  4.7012153]]\n",
      "Saving to checkpoint...\n",
      "[25] Average reward: 37.0\n",
      "Predicted reward: [[6.5754585 6.6686087]]\n",
      "Saving to checkpoint...\n",
      "[30] Average reward: 56.4\n",
      "Predicted reward: [[9.036702 9.099392]]\n",
      "Saving to checkpoint...\n",
      "[35] Average reward: 64.0\n",
      "Predicted reward: [[11.908424 11.792387]]\n",
      "Saving to checkpoint...\n",
      "[40] Average reward: 59.6\n",
      "Predicted reward: [[14.581057 14.813328]]\n",
      "Saving to checkpoint...\n",
      "[45] Average reward: 53.0\n",
      "Predicted reward: [[16.618753 16.970186]]\n",
      "Saving to checkpoint...\n",
      "[50] Average reward: 67.4\n",
      "Predicted reward: [[19.659374 19.202108]]\n",
      "Saving to checkpoint...\n",
      "[55] Average reward: 80.4\n",
      "Predicted reward: [[22.620201 22.76794 ]]\n",
      "Saving to checkpoint...\n",
      "[60] Average reward: 110.4\n",
      "Predicted reward: [[26.283184 26.978762]]\n",
      "Saving to checkpoint...\n",
      "[65] Average reward: 71.0\n",
      "Predicted reward: [[29.445484 29.528934]]\n",
      "Saving to checkpoint...\n",
      "[70] Average reward: 46.6\n",
      "Predicted reward: [[30.932318 31.416924]]\n",
      "Saving to checkpoint...\n",
      "[75] Average reward: 87.8\n",
      "Predicted reward: [[33.802704 34.204758]]\n",
      "Saving to checkpoint...\n",
      "[80] Average reward: 88.8\n",
      "Predicted reward: [[37.02757  37.181446]]\n",
      "Saving to checkpoint...\n",
      "[85] Average reward: 82.8\n",
      "Predicted reward: [[39.337654 40.39753 ]]\n",
      "Saving to checkpoint...\n",
      "[90] Average reward: 106.4\n",
      "Predicted reward: [[42.539394 42.601635]]\n",
      "Saving to checkpoint...\n",
      "[95] Average reward: 70.0\n",
      "Predicted reward: [[44.78354 44.82696]]\n",
      "Saving to checkpoint...\n",
      "[100] Average reward: 74.6\n",
      "Predicted reward: [[46.65727 46.4592 ]]\n",
      "Saving to checkpoint...\n",
      "[105] Average reward: 108.6\n",
      "Predicted reward: [[49.355083 49.47899 ]]\n",
      "Saving to checkpoint...\n",
      "[110] Average reward: 73.2\n",
      "Predicted reward: [[50.846523 51.317295]]\n",
      "Saving to checkpoint...\n",
      "[115] Average reward: 118.4\n",
      "Predicted reward: [[54.07593 53.3081 ]]\n",
      "Saving to checkpoint...\n",
      "[120] Average reward: 72.8\n",
      "Predicted reward: [[54.751564 55.628315]]\n",
      "Saving to checkpoint...\n",
      "[125] Average reward: 104.6\n",
      "Predicted reward: [[55.686577 56.481236]]\n",
      "Saving to checkpoint...\n",
      "[130] Average reward: 93.6\n",
      "Predicted reward: [[59.403927 57.48475 ]]\n",
      "Saving to checkpoint...\n",
      "[135] Average reward: 50.4\n",
      "Predicted reward: [[60.922287 59.339897]]\n",
      "Saving to checkpoint...\n",
      "[140] Average reward: 79.6\n",
      "Predicted reward: [[60.50973  61.235374]]\n",
      "Saving to checkpoint...\n",
      "[145] Average reward: 144.4\n",
      "Predicted reward: [[63.000595 63.56937 ]]\n",
      "Saving to checkpoint...\n",
      "[150] Average reward: 90.6\n",
      "Predicted reward: [[63.50826  63.772247]]\n",
      "Saving to checkpoint...\n",
      "[155] Average reward: 63.6\n",
      "Predicted reward: [[65.35979 64.9763 ]]\n",
      "Saving to checkpoint...\n",
      "[160] Average reward: 104.0\n",
      "Predicted reward: [[67.16278 66.43522]]\n",
      "Saving to checkpoint...\n",
      "[165] Average reward: 84.0\n",
      "Predicted reward: [[67.51614  68.203476]]\n",
      "Saving to checkpoint...\n",
      "[170] Average reward: 65.2\n",
      "Predicted reward: [[68.49412  70.202965]]\n",
      "Saving to checkpoint...\n",
      "[175] Average reward: 64.2\n",
      "Predicted reward: [[69.84264 68.73818]]\n",
      "Saving to checkpoint...\n",
      "[180] Average reward: 74.2\n",
      "Predicted reward: [[70.2495  70.79946]]\n",
      "Saving to checkpoint...\n",
      "[185] Average reward: 74.6\n",
      "Predicted reward: [[68.574165 70.94275 ]]\n",
      "Saving to checkpoint...\n",
      "[190] Average reward: 124.2\n",
      "Predicted reward: [[72.15542 73.02118]]\n",
      "Saving to checkpoint...\n",
      "[195] Average reward: 78.0\n",
      "Predicted reward: [[71.221405 72.45237 ]]\n",
      "Saving to checkpoint...\n",
      "[200] Average reward: 81.2\n",
      "Predicted reward: [[72.23124  73.886826]]\n",
      "Saving to checkpoint...\n",
      "[205] Average reward: 139.2\n",
      "Predicted reward: [[74.26295 73.36519]]\n",
      "Saving to checkpoint...\n",
      "[210] Average reward: 114.4\n",
      "Predicted reward: [[74.21004 74.48256]]\n",
      "Saving to checkpoint...\n",
      "[215] Average reward: 82.6\n",
      "Predicted reward: [[72.48575 74.26562]]\n",
      "Saving to checkpoint...\n",
      "[220] Average reward: 73.6\n",
      "Predicted reward: [[74.62599 74.72668]]\n",
      "Saving to checkpoint...\n",
      "[225] Average reward: 83.4\n",
      "Predicted reward: [[73.52508 74.00122]]\n",
      "Saving to checkpoint...\n",
      "[230] Average reward: 95.2\n",
      "Predicted reward: [[74.11505 74.27413]]\n",
      "Saving to checkpoint...\n"
     ]
    }
   ],
   "source": [
    "if action == 'train':\n",
    "    if algo == 'DQN':\n",
    "        agent.train(epochs=config['train_epochs'], t_max=t_max)\n",
    "    elif algo == 'PPO':\n",
    "        agent.train(epochs=config['train_epochs'], t_max=t_max, buf_size=3000, min_buf_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.get_model(agent.preprocess(env.reset())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rollout(t_max, env, close=True):\n",
    "    import sys\n",
    "    obs = agent.preprocess(env.reset())\n",
    "    reward = 0\n",
    "    for i in range(t_max):\n",
    "        # print(agent.get_policy(obs))\n",
    "        # act = agent.get_action(obs, greedy=True)[0]\n",
    "        act = agent.get_action(obs, mode='greedy')[0][0]\n",
    "        obs, r, dn, info = env.step(agent.action_wrapper(act))\n",
    "        env.render()\n",
    "        print(act, file=sys.stderr)\n",
    "        time.sleep(0.05)\n",
    "        obs = agent.preprocess(obs)\n",
    "        reward += r\n",
    "        if dn:\n",
    "            break\n",
    "\n",
    "    print(\"Total reward: {}\".format(reward), file=sys.stderr)\n",
    "    if close: env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'test':\n",
    "    test_rollout(10000, env, close=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.train(4, t_max=500, min_buf_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f agent.train agent.train(1, t_max=500, buf_size=2000, min_buf_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
