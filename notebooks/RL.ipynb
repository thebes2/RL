{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import gym_snake\n",
    "import json\n",
    "import importlib\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "os.chdir('..')\n",
    "from utils.Buffer import ReplayBuffer\n",
    "from utils.Conv import ConvHead\n",
    "from rl.models import get_policy_architecture, get_value_architecture, get_vision_architecture\n",
    "from algos.PPO import PPO_agent\n",
    "from algos.DQN import DQN_agent\n",
    "from utils.Loader import load_agent\n",
    "from utils.utils import *\n",
    "from utils.Env import get_env\n",
    "\n",
    "%matplotlib notebook\n",
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load('snake-model-zoo', 'snake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load('model_zoo/snake-03-06-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video_from_rollout(agent, agent.env, t_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = load('tetris-simple-7', 'tetris-simple', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(epochs=100, t_max=500, display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.eval()\n",
    "for _ in range(5):\n",
    "    print(agent.collect_rollout(t_max=1000, display=True, eval=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 5\n",
    "epochs = 100\n",
    "env = 'lunarlander'\n",
    "base_name = 'lunarlander-DQN-eps_anneal_compare2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = [\n",
    "    {\n",
    "        \"type\": \"InitBufferCallback\",\n",
    "        \"kwargs\": {\n",
    "            \"episodes\": 50\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"AnnealingSchedulerCallback\",\n",
    "        \"kwargs\": {\n",
    "            \"target\": \"epsilon\",\n",
    "            \"schedule\": [\n",
    "                {\n",
    "                    \"type\": \"Schedule\",\n",
    "                    \"kwargs\": {\n",
    "                        \"length\": 200,\n",
    "                        \"start_val\": 0.4,\n",
    "                        \"end_val\": 0.01,\n",
    "                        \"fn\": \"linear\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1, hist2 = compare_algos(base_name, runs, epochs, env, \n",
    "    # dict({'algo': ['DDQN', 'PER', 'Dueling']})\n",
    "    dict({'callbacks': cb})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runs(hist1, hist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_runs(hist1, hist2) # blue is hist1, orange is hist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tetris = importlib.import_module('pytris-effect.src.gameui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'snake'\n",
    "action = 'train'\n",
    "algo = ('DDQN', 'Dueling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_fp = os.path.join('..', 'configs', run_name + '.json')\n",
    "with open(cfg_fp, 'r') as f:\n",
    "    config = json.load(f)\n",
    "ckpt_folder = os.path.join('..', 'checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = config['env']\n",
    "if run_name == 'tetris':\n",
    "    env = tetris.GameUI(graphic_mode=False, its_per_sec=2, sec_per_tick=0.5)\n",
    "else:\n",
    "    env = gym.make(env_name).env if 'use_raw_env' in config else gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(arr, scaling=30):\n",
    "    data = np.zeros((scaling*arr.shape[0], scaling*arr.shape[1], 3), dtype=np.uint8)\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            for k in range(data.shape[2]):\n",
    "                data[i,j,k] = arr[i//scaling,j//scaling,k]\n",
    "    img = Image.fromarray(data, 'RGB')\n",
    "    # img.save('my.png')\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'evaluate':\n",
    "    %lprun -f env.drawMatrix env.drawMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'evaluate':\n",
    "    arr = env.reset()[::10,::10,:]\n",
    "    img = Image.fromarray(arr, 'RGB')\n",
    "    img.show()\n",
    "    #show_img(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    action = 1\n",
    "    obs, reward, dn, info = env.step(action)\n",
    "    show_img(obs)\n",
    "    print(reward, dn, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_step():\n",
    "    _, _, dn, _ = env.step(random.choice(range(7)))\n",
    "    if dn:\n",
    "        env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit do_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%lprun -f env.get_obs do_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_policy_architecture(env_name, algo=algo)\n",
    "if 'DQN' in \"\\n\".join(algo):\n",
    "    target = tf.keras.models.clone_model(model)\n",
    "else:\n",
    "    value = get_value_architecture(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'DQN' in \"\\n\".join(algo):\n",
    "    agent = DQN_agent(\n",
    "        model,\n",
    "        ReplayBuffer(config.get(\"max_buf_size\", 20000), mode='uniform'),\n",
    "        target=target,\n",
    "        env=env,\n",
    "        mode=('DDQN'), # 'PER'\n",
    "        learning_rate=config['learning_rate'],\n",
    "        batch_size=config['batch_size'],\n",
    "        update_steps=1,\n",
    "        update_freq=4,\n",
    "        multistep=5,\n",
    "        alpha=1.5,\n",
    "        beta=1.0,\n",
    "        gamma=0.95,\n",
    "        target_delay=1000,\n",
    "        delta=1.0,\n",
    "        # delta=0.000003,\n",
    "        env_name=config['env_name'],\n",
    "        algo_name='DQN',\n",
    "        ckpt_folder=ckpt_folder,\n",
    "        run_name='snake-DQN-pretrain-hard_update-uniform-multistep5-7'\n",
    "    )\n",
    "elif 'PPO' in \"\\n\".join(algo):\n",
    "    agent = PPO_agent(\n",
    "        model,\n",
    "        value,\n",
    "        env=env,\n",
    "        learning_rate=config['learning_rate'],\n",
    "        minibatch_size=config['minibatch_size'],\n",
    "        gamma=0.99,\n",
    "        env_name=config['env_name'],\n",
    "        run_name='snake-PPO-pretrain',\n",
    "        ckpt_folder=ckpt_folder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max = config['t_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_buf = []\n",
    "\n",
    "def collect_rollout(env, t_max, policy):\n",
    "    s = agent.preprocess(env.reset())\n",
    "    for t in range(t_max):\n",
    "        act = policy(s)\n",
    "        ss, r, dn, _ = env.step(agent.action_wrapper(act))\n",
    "        ss = agent.preprocess(ss)\n",
    "        p_buf.append([s, ss])\n",
    "        s = ss\n",
    "        if dn:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = True\n",
    "if pretrain and action == 'train': # only necessary for tasks on raw pixels (vision)\n",
    "    model = get_vision_architecture(env_name)\n",
    "    out = tf.keras.layers.Dense(16, activation=None)(model.output)\n",
    "    embed = tf.keras.Model(inputs=model.input, outputs=out)\n",
    "    # get some data from random interactions with the env\n",
    "    for i in tqdm(range(500)):\n",
    "        collect_rollout(env, t_max, lambda x: np.random.choice(4))\n",
    "    print(\"Collected {} samples\".format(len(p_buf)))\n",
    "    head = ConvHead(embed, p_buf)\n",
    "    head.train(6)\n",
    "    \n",
    "    out = head.model.layers[-2].output\n",
    "    vision = tf.keras.Model(inputs=head.model.input, outputs=out)\n",
    "    pretrained_model = get_policy_architecture(env_name, algo=algo, head=vision)\n",
    "    agent.set_model(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = get_policy_architecture(env_name, algo=algo, head=vision)\n",
    "agent.set_model(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_from_checkpoint()\n",
    "hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'train':\n",
    "    if 'DQN' in \"\\n\".join(algo):\n",
    "        # fill buffer with some random samples\n",
    "        for i in tqdm(range(500)):\n",
    "            agent.collect_rollout(t_max=t_max, policy=lambda x: np.random.choice(4), train=False, display=False)\n",
    "        #print(agent.epsilon)\n",
    "        #agent.epsilon = 0.05\n",
    "        hist += agent.train(epochs=config['train_epochs'], t_max=t_max, display=False)\n",
    "    elif 'PPO' in \"\\n\".join(algo):\n",
    "        agent.train(epochs=config['train_epochs'], t_max=t_max, buf_size=3000, min_buf_size=600, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "a = hist[::1]\n",
    "plt.plot(range(len(a)), a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rollout(t_max, env, close=True):\n",
    "    import sys\n",
    "    obs = agent.preprocess(env.reset())\n",
    "    reward = 0\n",
    "    for i in range(t_max):\n",
    "        # print(agent.get_policy(obs))\n",
    "        # act = agent.get_action(obs, greedy=True)[0]\n",
    "        act = agent.get_action(obs, mode='greedy')[0][0]\n",
    "        obs, r, dn, info = env.step(agent.action_wrapper(act))\n",
    "        env.render()\n",
    "        print(act, file=sys.stderr)\n",
    "        time.sleep(0.05)\n",
    "        obs = agent.preprocess(obs)\n",
    "        reward += r\n",
    "        if dn:\n",
    "            break\n",
    "\n",
    "    print(\"Total reward: {}\".format(reward), file=sys.stderr)\n",
    "    if close: env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if action == 'test':\n",
    "    test_rollout(10000, env, close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.train(4, t_max=500, min_buf_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f agent.train agent.train(1, t_max=500, buf_size=2000, min_buf_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
